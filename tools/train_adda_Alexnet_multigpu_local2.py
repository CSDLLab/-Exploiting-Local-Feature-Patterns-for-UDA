import logging
import os
import random
from collections import deque
from collections import OrderedDict

import click
import numpy as np
import tensorflow as tf
from tensorflow.contrib import slim
from tqdm import tqdm

import adda


@click.command()
@click.argument('source')
@click.argument('target')
@click.argument('model')
@click.argument('output')
@click.option('--gpu', default='0')
@click.option('--iterations', default=20000)
@click.option('--batch_size', default=50)
@click.option('--display', default=10)
@click.option('--lr', default=1e-4)
@click.option('--stepsize', type=int)
@click.option('--snapshot', default=5000)
@click.option('--netvladflag', type=int)
@click.option('--weights', required=True)
@click.option('--solver', default='sgd')
@click.option('--adversary', 'adversary_layers', default=[500, 500],
              multiple=True)
@click.option('--adversary_leaky/--adversary_relu', default=True)
@click.option('--seed', type=int)
def main(source, target, model, output,
         gpu, iterations, batch_size, display, lr, stepsize, snapshot, weights,
         solver, adversary_layers, adversary_leaky, seed, netvladflag):
    # miscellaneous setup
    adda.util.config_logging()
    if 'CUDA_VISIBLE_DEVICES' in os.environ:
        logging.info('CUDA_VISIBLE_DEVICES specified, ignoring --gpu flag')
    else:
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu
    logging.info('Using GPU {}'.format(os.environ['CUDA_VISIBLE_DEVICES']))
    if seed is None:
        seed = random.randrange(2 ** 32 - 2)
    logging.info('Using random seed {}'.format(seed))
    random.seed(seed)
    np.random.seed(seed + 1)
    tf.set_random_seed(seed + 2)
    error = False

    def average_gradients(tower_grads):
        average_grads = []
        for grad_and_vars in zip(*tower_grads):
            grads = []
            for g, _ in grad_and_vars:
                expanded_g = tf.expand_dims(g, 0)
                grads.append(expanded_g)

            grad = tf.concat(grads, 0)
            grad = tf.reduce_mean(grad, 0)
            v = grad_and_vars[0][1]
            grad_and_var = (grad, v)
            average_grads.append(grad_and_var)
        return average_grads

    def inference(netvladflag, source_im_batch, target_im_batch, model_fn):

        source_ft, layers_source = model_fn(source_im_batch, scope='source')
        target_ft, layers_target = model_fn(target_im_batch, scope='target')
        if True:
            print("______vlad______")
            netvlad_alpha = 1.0
            l2_norm_flag = True
            n_clusters = 32
            vlad_channel_num = 256
            vlad_layer = 'conv5'
            with tf.variable_scope('NetVLAD'):
                cluster_centers = np.random.normal(size=(n_clusters, vlad_channel_num), loc=30.0, scale=10.0, )
                vlad_centers_variable = slim.model_variable(
                    'centers_vlad',
                    shape=cluster_centers.shape,
                    initializer=tf.constant_initializer(cluster_centers))
                vlad_W = tf.expand_dims(tf.expand_dims(tf.transpose(vlad_centers_variable) * 2 * netvlad_alpha, axis=0),
                                        axis=0)
                vlad_B = tf.reduce_sum(tf.square(vlad_centers_variable), axis=1) * (netvlad_alpha) * (-1)
                vlad_input_source = layers_source[vlad_layer]
                vlad_input_target = layers_target[vlad_layer]
                vlad_rep_output_s, assgn_s, loss_vladsparse_s, vlad_centers = \
                    adda.util.netvlad(vlad_input_source, vlad_centers=vlad_centers_variable, scope="vladconv",
                                      vlad_W=vlad_W, vlad_B=vlad_B, netvlad_alpha=netvlad_alpha,
                                      netvlad_initCenters=n_clusters,
                                      l2_norm_flag=l2_norm_flag, reuse=False)

                vlad_rep_output_t, assgn_t, loss_vladsparse_t, vlad_centers = \
                    adda.util.netvlad(vlad_input_target, vlad_centers=vlad_centers_variable, scope="vladconv",
                                      vlad_W=vlad_W, vlad_B=vlad_B, netvlad_alpha=netvlad_alpha,
                                      netvlad_initCenters=n_clusters,
                                      l2_norm_flag=l2_norm_flag, reuse=True)
                vlad_rep_output_s = slim.fully_connected(vlad_rep_output_s, 512, scope='vladfcn1')
                source_ft = slim.fully_connected(vlad_rep_output_s, 31, activation_fn=None, scope='vladfcn2')
                vlad_rep_output_t = slim.fully_connected(vlad_rep_output_t, 512, scope='vladfcn1', reuse=True)
                target_ft = slim.fully_connected(vlad_rep_output_t, 31, activation_fn=None, scope='vladfcn2',
                                                 reuse=True)
                # ### using the residual with the vlad center for alignment
                # print ("vlad_input_source:",vlad_input_source)
                # vlad_centers_variable_expand=tf.expand_dims(vlad_centers_variable, axis=0)
                # vlad_center_pack=tf.concat([vlad_centers_variable_expand,vlad_centers_variable_expand],axis=0)
                # for packi in range(int(vlad_input_source.get_shape()[0])*
                #                    int(vlad_input_source.get_shape()[1])*int(vlad_input_source.get_shape()[2])-2):
                #     vlad_center_pack = tf.concat([vlad_center_pack, vlad_centers_variable_expand], axis=0)
                # vlad_center_pack=tf.reshape(vlad_center_pack,[int(vlad_input_source.get_shape()[0]),
                #                             int(vlad_input_source.get_shape()[1]),
                #                         int(vlad_input_source.get_shape()[2]),
                #                             n_clusters, vlad_channel_num])



                #vlad_input_source_temp=vlad_input_source-vlad_center_pack[tf.argmax(assgn_s, axis=3)]
                #vlad_input_target_temp = vlad_input_target - vlad_center_pack[tf.argmax(assgn_t, axis=3)]

                downsample_size=(6, 6)
                assgn_s_arg = tf.argmax(assgn_s, axis=3)
                assgn_t_arg = tf.argmax(assgn_t, axis=3)
                print("-----------assgn_s_arg1:", assgn_s_arg)

                assgn_s_arg=tf.one_hot(assgn_s_arg,depth=n_clusters)
                assgn_t_arg = tf.one_hot(assgn_t_arg, depth=n_clusters)
                print("-----------assgn_s_arg2:", assgn_s_arg)

                assgn_s_arg = tf.image.resize_nearest_neighbor(assgn_s_arg, downsample_size)
                assgn_t_arg = tf.image.resize_nearest_neighbor(assgn_t_arg, downsample_size)
                vlad_input_source = tf.image.resize_nearest_neighbor(vlad_input_source, downsample_size)
                vlad_input_target = tf.image.resize_nearest_neighbor(vlad_input_target, downsample_size)
                print("-----------assgn_s_arg3:", assgn_s_arg)
                vlad_input_source = tf.reshape(vlad_input_source, [-1, int(vlad_input_source.get_shape()[-1])])
                vlad_input_target = tf.reshape(vlad_input_target, [-1, int(vlad_input_target.get_shape()[-1])])
                assgn_s_arg = tf.reshape(assgn_s_arg, [-1, int(assgn_s_arg.get_shape()[-1])])
                assgn_t_arg = tf.reshape(assgn_t_arg, [-1, int(assgn_t_arg.get_shape()[-1])])

                print("-----------vlad_input_source:", vlad_input_source)

                source_ft_local = vlad_input_source-tf.matmul(assgn_s_arg,vlad_centers_variable)
                target_ft_local = vlad_input_target-tf.matmul(assgn_t_arg,vlad_centers_variable)

                source_ft_local = tf.concat([source_ft_local, assgn_s_arg], axis=1)
                target_ft_local = tf.concat([target_ft_local, assgn_t_arg], axis=1)

                print("-----------assgn_s_arg4:", assgn_s_arg)
                print("-----------target_ft_local----------:", target_ft_local)


                '''  ### using the assigement as for alignment
                assgn_s = tf.image.resize_nearest_neighbor(assgn_s, (5, 5))
                assgn_t = tf.image.resize_nearest_neighbor(assgn_t, (5, 5))
                assgn_s_arg = tf.expand_dims(tf.cast(tf.argmax(assgn_s, axis=3), "float32"), axis=3)
                assgn_t_arg = tf.expand_dims(tf.cast(tf.argmax(assgn_t, axis=3), "float32"), axis=3)
                #assgn_s_average = tf.reduce_mean(assgn_s, axis=[1, 2])
                #assgn_t_average = tf.reduce_mean(assgn_t, axis=[1, 2])

                source_ft_local = tf.concat([assgn_s_arg, assgn_s], axis=3)
                target_ft_local = tf.concat([assgn_t_arg, assgn_t], axis=3)
                '''
                print("source_ft_local", source_ft_local)

        if True:
            source_ft_local = tf.reshape(source_ft_local, [-1, int(source_ft_local.get_shape()[-1])])
            target_ft_local = tf.reshape(target_ft_local, [-1, int(target_ft_local.get_shape()[-1])])
            adversary_ft_local = tf.concat([source_ft_local, target_ft_local], 0)
            source_adversary_label_local = tf.zeros([tf.shape(source_ft_local)[0]], tf.int32)
            target_adversary_label_local = tf.ones([tf.shape(target_ft_local)[0]], tf.int32)
            adversary_label_local = tf.concat(
                [source_adversary_label_local, target_adversary_label_local], 0)
            adversary_logits_local, adversary_layerfeature_local = adda.adversary.adversarial_discriminator(
                adversary_ft_local, [768, 512], leaky=True, scope="local_adversary")

        source_ft = tf.reshape(source_ft, [-1, int(source_ft.get_shape()[-1])])
        target_ft = tf.reshape(target_ft, [-1, int(target_ft.get_shape()[-1])])
        adversary_ft = tf.concat([source_ft, target_ft], 0)
        source_adversary_label = tf.zeros([tf.shape(source_ft)[0]], tf.int32)
        target_adversary_label = tf.ones([tf.shape(target_ft)[0]], tf.int32)
        adversary_label = tf.concat(
            [source_adversary_label, target_adversary_label], 0)
        adversary_logits, adversary_layerfeature = adda.adversary.adversarial_discriminator(
            net=adversary_ft, layers=[2048, 1024], leaky=True, scope="global_adversary")

        mapping_loss = tf.losses.sparse_softmax_cross_entropy(
            1 - adversary_label, adversary_logits)
        adversary_loss = tf.losses.sparse_softmax_cross_entropy(
            adversary_label, adversary_logits)
        mapping_loss_mmd = adda.util.mmd_loss(source_ft, target_ft, 1.0)

        mapping_loss_mmd_local = adda.util.mmd_loss(source_ft_local, target_ft_local, 1.0)

        mapping_loss_local = tf.losses.sparse_softmax_cross_entropy(
            1 - adversary_label_local, adversary_logits_local)
        adversary_loss_local = tf.losses.sparse_softmax_cross_entropy(
            adversary_label_local, adversary_logits_local)
        return mapping_loss, adversary_loss, mapping_loss_mmd,mapping_loss_local,adversary_loss_local,mapping_loss_mmd_local

    with tf.Graph().as_default(), tf.device('/cpu:0'):
        try:
            source_dataset_name, source_split_name = source.split(':')
        except ValueError:
            error = True
            logging.error(
                'Unexpected source dataset {} (should be in format dataset:split)'
                    .format(source))
        try:
            target_dataset_name, target_split_name = target.split(':')
        except ValueError:
            error = True
            logging.error(
                'Unexpected target dataset {} (should be in format dataset:split)'
                    .format(target))
        if error:
            raise click.Abort

        # setup data
        logging.info('Adapting {} -> {}'.format(source, target))
        source_dataset = getattr(adda.data.get_dataset(source_dataset_name),
                                 source_split_name)
        target_dataset = getattr(adda.data.get_dataset(target_dataset_name),
                                 target_split_name)
        source_im, source_label = source_dataset.tf_ops()
        target_im, target_label = target_dataset.tf_ops()
        model_fn = adda.models.get_model_fn(model)
        source_im = adda.models.preprocessing(source_im, model_fn)
        target_im = adda.models.preprocessing(target_im, model_fn)
        lr_var = tf.Variable(lr, name='learning_rate', trainable=False)
        if solver == 'sgd':
            optimizer = tf.train.MomentumOptimizer(lr_var, 0.99)
        else:
            optimizer = tf.train.AdamOptimizer(lr_var, 0.5)

        optimizer_adver_global = tf.train.AdamOptimizer(lr_var*1.0, 0.5)
        optimizer_adver_local = tf.train.AdamOptimizer(lr_var*1.0, 0.5)
        optimizer_target = tf.train.AdamOptimizer(lr_var, 0.5)
        global_step = tf.get_variable(
            'global_step', [],
            initializer=tf.constant_initializer(0), trainable=False)

        tower_grads_adver_global = []
        tower_grads_adver_local = []
        tower_grads_target = []
        mapping_loss = []
        adversary_loss = []
        mapping_loss_local = []
        adversary_loss_local = []
        mapping_loss_mmd = []
        mapping_loss_mmd_local = []
        gpu_visible = [0,1]
        optimizer = tf.train.AdamOptimizer(lr_var, 0.5)
        with tf.variable_scope(tf.get_variable_scope()):
            for gpui in range(len(gpu_visible)):
                with tf.device('/gpu:%d' % gpu_visible[gpui]):
                    # with tf.variable_scope('Tower_%d' % (gpu_visible[gpui]))
                    with tf.name_scope('Tower_%d' % (gpu_visible[gpui])) as scope:
                        # with tf.variable_scope('Tower_%d' % (gpu_visible[gpui]), reuse=tf.AUTO_REUSE) as scope:
                        source_im_batch, source_label_batch = tf.train.batch(
                            [source_im, source_label], batch_size=batch_size)
                        target_im_batch, target_label_batch = tf.train.batch(
                            [target_im, target_label], batch_size=batch_size)

                        mapping_loss_temp, adversary_loss_temp, mapping_loss_mmd_temp,\
                        mapping_loss_local_temp,adversary_loss_local_temp,mapping_loss_mmd_local_temp \
                            = inference(netvladflag, source_im_batch, target_im_batch, model_fn)

                        mapping_loss_total_temp = 0.6*mapping_loss_temp + 0.0*mapping_loss_mmd_temp+0.4*mapping_loss_local_temp
                        train_variables = tf.trainable_variables()
                        target_variables = [v for v in train_variables if (("target" in v.name) and
                                            (("conv5" in v.name)  or ("fdfadf" in v.name)))]
                        adversary_vars = [v for v in train_variables if
                                          ("global_adversary" in v.name) or ("aaatestaaa" in v.name)]
                        adversary_vars_local = [v for v in train_variables if
                                                ("local_adversary" in v.name) or ("aaatestaaa" in v.name)]

                        tf.get_variable_scope().reuse_variables()
                        grads_adver_global = optimizer_adver_global.compute_gradients(adversary_loss_temp, var_list=adversary_vars)
                        grads_adver_local = optimizer_adver_local.compute_gradients(adversary_loss_local_temp,
                                                                         var_list=adversary_vars_local)
                        grads_target = optimizer_target.compute_gradients(mapping_loss_total_temp, var_list=target_variables)

                        # Keep track of the gradients across all towers.
                        tower_grads_adver_global.append(grads_adver_global)
                        # tower_grads_adver_local.append(grads_adver_local)
                        tower_grads_target.append(grads_target)
                        tower_grads_adver_local.append(grads_adver_local)

                        mapping_loss.append(mapping_loss_temp)
                        adversary_loss.append(adversary_loss_temp)

                        mapping_loss_local.append(mapping_loss_local_temp)
                        adversary_loss_local.append(adversary_loss_local_temp)

                        mapping_loss_mmd.append(mapping_loss_mmd_temp)
                        mapping_loss_mmd_local.append(mapping_loss_mmd_local_temp)

        grads_target = average_gradients(tower_grads_target)
        grads_adver_global = average_gradients(tower_grads_adver_global)
        grads_adver_local = average_gradients(tower_grads_adver_local)

        mapping_loss=tf.reduce_mean(mapping_loss)
        adversary_loss=tf.reduce_mean(adversary_loss)
        mapping_loss_mmd = tf.reduce_mean(mapping_loss_mmd)

        mapping_loss_local = tf.reduce_mean(mapping_loss_local)
        adversary_loss_local = tf.reduce_mean(adversary_loss_local)
        mapping_loss_mmd_local = tf.reduce_mean(mapping_loss_mmd_local)

        # apply the gradients with our optimizers

        mapping_step = optimizer_target.apply_gradients(grads_target, global_step=global_step)
        adversary_step = optimizer_adver_global.apply_gradients(grads_adver_global, global_step=global_step)
        adversary_step_local = optimizer.apply_gradients(grads_adver_local, global_step=global_step)

        var_dict_vlad_only = adda.util.collect_vars("NetVLAD")
        var_dict_target = adda.util.collect_vars("target")
        var_dict_source = adda.util.collect_vars('source')

        var_dict_encoder_vlad = var_dict_source.copy()
        var_dict_encoder_vlad.update(var_dict_vlad_only)

        var_dict_save = var_dict_target.copy()
        var_dict_save.update(var_dict_vlad_only)

        # set up session and initialize
        init = tf.global_variables_initializer()
        config = tf.ConfigProto(allow_soft_placement=True)
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        sess.run(init)

        # restore weights
        if os.path.isdir(weights):
            weights = tf.train.latest_checkpoint(weights)

        # logging.info('Restoring weights from {}:'.format(weights))
        # logging.info('    Restoring source model:')
        # source_restorer = tf.train.Saver(var_list=var_dict_encoder_vlad)
        # source_restorer.restore(sess, weights)
        #
        # logging.info('    Restoring save model:')
        # target_restorer_pre = tf.train.Saver(var_list=var_dict_save)
        # target_restorer_pre.restore(sess, weights)


        logging.info('    Restoring var_dict_encoder_vlad model:')
        for src, tgt in var_dict_encoder_vlad.items():
            logging.info('        {:30} -> {:30}'.format(src, tgt.name))
        target_restorer_pre = tf.train.Saver(var_list=var_dict_encoder_vlad)
        target_restorer_pre.restore(sess, weights)


        '''
        '''
        logging.info('    Restoring save model:')
        for src, tgt in var_dict_save.items():
            logging.info('        {:30} -> {:30}'.format(src, tgt.name))
        decoder_restorer = tf.train.Saver(var_list=var_dict_save)
        decoder_restorer.restore(sess, weights)


        target_saver = tf.train.Saver(var_list=var_dict_save)
        # optimization loop (finally)
        output_dir = os.path.join('snapshot', output)
        if not os.path.exists(output_dir):
            os.mkdir(output_dir)
        mapping_losses = deque(maxlen=10)
        adversary_losses = deque(maxlen=10)
        bar = tqdm(range(iterations))
        bar.set_description('{} (lr: {:.0e})'.format(output, lr))
        bar.refresh()
        for i in bar:
            if netvladflag > 0:
                if i <-120:
                    _= sess.run([ adversary_step])
                else:
                    _, _,_= sess.run([mapping_step, adversary_step,adversary_step_local])

                if i % display == 0:
                    mapping_loss_val, adversary_loss_val, mapping_loss_mmd_val, \
                    mapping_loss_local_val, mapping_loss_mmd_local_val, adversary_loss_local_val= sess.run(
                        [mapping_loss, adversary_loss, mapping_loss_mmd,
                         mapping_loss_local,mapping_loss_mmd_local,adversary_loss_local])
                    mapping_losses.append(mapping_loss_val)
                    adversary_losses.append(adversary_loss_val)

                    logging.info('{:4} Mapping: {:4.2f}'
                                 ' Adversary: {:4.2f}  loss_mmd: {:4.2f} '
                                 ' Mapping_local: {:4.2f}  loss_mmd_local: {:4.2f} Adver_local: {:4.2f} '
                                 .format('Iteration {}:'.format(i),
                                         mapping_loss_val,
                                         adversary_loss_val, mapping_loss_mmd_val,
                                         mapping_loss_local_val, mapping_loss_mmd_local_val, adversary_loss_local_val))
            else:
                mapping_loss_val, adversary_loss_val, _, _ = sess.run(
                    [mapping_loss, adversary_loss, mapping_step, adversary_step])
                mapping_losses.append(mapping_loss_val)
                adversary_losses.append(adversary_loss_val)
                if i % display == 0:
                    logging.info('{:10} Mapping: {:8.4f}     (avg: {:8.4f})'
                                 '    Adversary: {:8.4f}     (avg: {:8.4f})'
                                 .format('Iteration {}:'.format(i),
                                         mapping_loss_val,
                                         np.mean(mapping_losses),
                                         adversary_loss_val,
                                         np.mean(adversary_losses)))

            if stepsize is not None and (i + 1) % stepsize == 0:
                lr = sess.run(lr_var.assign(lr * 0.1))
                logging.info('Changed learning rate to {:.0e}'.format(lr))
                bar.set_description('{} (lr: {:.0e})'.format(output, lr))
            if (i + 1) % snapshot == 0:
                snapshot_path = target_saver.save(
                    sess, os.path.join(output_dir, output), global_step=i + 1)
                logging.info('Saved snapshot to {}'.format(snapshot_path))

        coord.request_stop()
        coord.join(threads)
        sess.close()


if __name__ == '__main__':
    main()
