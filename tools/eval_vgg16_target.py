import logging
import os
import time
from collections import OrderedDict

import click
import numpy as np
import tensorflow as tf
from tqdm import tqdm

import adda

from tensorflow.contrib import slim


def format_array(arr):
    return '  '.join(['{:.3f}'.format(x) for x in arr])


@click.command()
@click.argument('dataset')
@click.argument('split')
@click.argument('model')
@click.option('--netvladflag', type=int)
@click.option('--poolinglayer_mode', type=int,default=2)
@click.option('--cluster_number', type=int,default=32)
@click.argument('weights')
@click.option('--gpu', default='0')
def main(dataset, split, model, weights, gpu, netvladflag,poolinglayer_mode,cluster_number):
    adda.util.config_logging()
    if 'CUDA_VISIBLE_DEVICES' in os.environ:
        logging.info('CUDA_VISIBLE_DEVICES specified, ignoring --gpu flag')
    else:
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu
    logging.info('Using GPU {}'.format(os.environ['CUDA_VISIBLE_DEVICES']))

    print("dataset:", dataset)
    if dataset == "art" or dataset == "realworld" or dataset == "clipart" or dataset == "product":
        print("____using officehome dataset:", dataset)
        class_number = 65
    else:
        print("____using office31 dataset:", dataset)
        class_number = 31


    if poolinglayer_mode==0:
        vladfinetune_layer=["conv4_3","conv5_1"]
    elif poolinglayer_mode==1:
        vladfinetune_layer = ["conv5_1","conv5_2"]
    elif poolinglayer_mode == 2:
        vladfinetune_layer = ["conv5_2","conv5_3"]
    elif poolinglayer_mode==3:
        vladfinetune_layer = ["conv5_3","fc6"]
    elif poolinglayer_mode==4:
        vladfinetune_layer = ["conv5_3","fc7"]
    else:
        print ("______wrong poolinglayer_mode_____ ")
        vladfinetune_layer = ["conv5_2","conv5_3"]

    dataset_name = dataset
    split_name = split
    dataset = adda.data.get_dataset(dataset, shuffle=False)
    split = getattr(dataset, split)
    model_fn = adda.models.get_model_fn(model)
    im, label = split.tf_ops(capacity=2)
    im = adda.models.preprocessing(im, model_fn)



    im_batch, label_batch = tf.train.batch([im, label], batch_size=1)

    net, layers = model_fn(im_batch, scope='vgg16_target',class_number=class_number)

    vlad_fcn_flag = False
    vlad_WB_flag=False


    if netvladflag > 0:
        netvlad_alpha = 1.0
        l2_norm_flag = False
        n_clusters = cluster_number
        vlad_channel_num = 512
        vlad_layer = vladfinetune_layer[1]
        print("______vlad______")
        with tf.variable_scope('NetVLAD'):
            cluster_centers = np.random.normal(size=(n_clusters, vlad_channel_num), loc=30.0, scale=10.0, )
            vlad_centers_variable = slim.model_variable(
                'centers_vlad',
                shape=cluster_centers.shape,
                initializer=tf.constant_initializer(cluster_centers))

            if vlad_WB_flag==True:
                vlad_W = slim.model_variable('vlad_W',shape=(1, 1,) + cluster_centers.transpose().shape,
                                             initializer=tf.constant_initializer(cluster_centers.transpose()[np.newaxis, np.newaxis, ...] *
                                                 2 * netvlad_alpha))
                vlad_B = slim.model_variable( 'vlad_B', shape=cluster_centers.shape[0],
                    initializer=tf.constant_initializer(
                        -netvlad_alpha *np.sum(np.square(cluster_centers), axis=1)))
            else:
                vlad_W = tf.expand_dims(tf.expand_dims(tf.transpose(vlad_centers_variable) * 2 * netvlad_alpha, axis=0),
                                    axis=0)
                vlad_B = tf.reduce_sum(tf.square(vlad_centers_variable), axis=1) * (netvlad_alpha) * (-1)

            vlad_input = layers[vlad_layer]
            if vlad_layer == "fc3" or vlad_layer == 'fc4':
                vlad_input = tf.expand_dims(vlad_input, 1)
                vlad_input = tf.expand_dims(vlad_input, 1)
            vlad_rep_output, assgns, loss_vlad_sparse, vlad_centers = \
                adda.util.netvlad(vlad_input, vlad_centers=vlad_centers_variable, scope="vladconv",
                                  vlad_W=vlad_W, vlad_B=vlad_B, netvlad_alpha=netvlad_alpha,
                                  netvlad_initCenters=n_clusters,
                                  l2_norm_flag=l2_norm_flag)
            if vlad_fcn_flag == True:
                vlad_rep_output = slim.fully_connected(vlad_rep_output, 256, scope='vladfcn1')
            assgns_arg = tf.cast(tf.argmax(assgns, axis=3), "float32")
            assgns_arg = tf.reshape(assgns_arg, [int(assgns_arg.get_shape()[0]), -1])
            print("---assgns_arg:", assgns_arg)
            layer_conv3 = tf.reshape(layers[vlad_layer], [-1, int(layers[vlad_layer].get_shape()[-1])])
            net_vlad = slim.fully_connected(vlad_rep_output,class_number, activation_fn=None, scope='vladfcn2')
        net = net_vlad



    net = tf.argmax(net, -1)
    print ("_______net:",net)

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.Session(config=config)
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    sess.run(tf.global_variables_initializer())
    # var_dict = adda.util.collect_vars(model)

    var_dict_vlad_only = adda.util.collect_vars("NetVLAD")
    var_dict_vgg16 = adda.util.collect_vars('vgg16_target')

    var_dict_all = var_dict_vgg16.copy()
    var_dict_all.update(var_dict_vlad_only)


    # restorer = tf.train.Saver(var_list=var_dict_all)
    # if os.path.isdir(weights):
    #     weights = tf.train.latest_checkpoint(weights)
    # logging.info('Evaluating {}'.format(weights))
    # restorer.restore(sess, weights)

    logging.info('    Restoring target model:')
    for src, tgt in var_dict_all.items():
        logging.info('        {:30} -> {:30}'.format(src, tgt.name))
    restorer = tf.train.Saver(var_list=var_dict_all)
    weights = tf.train.latest_checkpoint(weights)
    restorer.restore(sess, weights)

    class_correct = np.zeros(dataset.num_classes, dtype=np.int32)
    class_counts = np.zeros(dataset.num_classes, dtype=np.int32)
    for i in tqdm(range(int(len(split) / 1))):
        predictions, gt = sess.run([net, label_batch])
        class_counts[gt[0]] += 1
        if predictions[0] == gt[0]:
            class_correct[gt[0]] += 1
    logging.info('Class accuracies:')
    logging.info('    ' + format_array(class_correct / class_counts))
    logging.info('Overall accuracy:')
    logging.info('    ' + str(np.sum(class_correct) / np.sum(class_counts)))

    coord.request_stop()
    coord.join(threads)
    sess.close()


if __name__ == '__main__':
    main()
